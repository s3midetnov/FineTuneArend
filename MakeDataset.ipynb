{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T12:39:05.725341Z",
     "start_time": "2025-11-26T12:39:05.718852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def parse_concatenated_json(path):\n",
    "    objects = []\n",
    "    buffer = []\n",
    "    brace_level = 0\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            # Count opening/closing braces not inside strings\n",
    "            brace_level += line.count(\"{\")\n",
    "            brace_level -= line.count(\"}\")\n",
    "\n",
    "            buffer.append(line)\n",
    "\n",
    "            # When brace level drops to zero we have a complete JSON object\n",
    "            if brace_level == 0 and buffer:\n",
    "                block = \"\".join(buffer).strip()\n",
    "                if block:\n",
    "                    objects.append(json.loads(block))\n",
    "                buffer = []\n",
    "\n",
    "    return objects\n"
   ],
   "id": "a94be2d13eaf37f",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T12:39:08.891995Z",
     "start_time": "2025-11-26T12:39:07.567384Z"
    }
   },
   "cell_type": "code",
   "source": "x = parse_concatenated_json('/Users/artem.semidetnov/Documents/Predictor/data.json')",
   "id": "a2b70717e83b5f48",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T12:39:13.012292Z",
     "start_time": "2025-11-26T12:39:13.008319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def simplify_jsonl(entry : dict) -> dict:\n",
    "    two_field_dict = dict()\n",
    "    two_field_dict['prompt'] = str(entry['Context']) + \"<<<break>>>\" + str(entry['Premises']) + '<<<break>>>' + entry['Expected type']\n",
    "    two_field_dict['completion'] = entry['Expression']\n",
    "    return two_field_dict"
   ],
   "id": "f1bf19df79768904",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T12:39:37.752568Z",
     "start_time": "2025-11-26T12:39:36.769354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "path_train = '/Users/artem.semidetnov/Documents/Predictor/train2/train.jsonl'\n",
    "path_valid = '/Users/artem.semidetnov/Documents/Predictor/train2/valid.jsonl'\n",
    "path_test = '/Users/artem.semidetnov/Documents/Predictor/train2/test.jsonl'\n",
    "\n",
    "train_sample = random.sample(x, 30000)\n",
    "with open(path_train, 'w', encoding='utf-8') as f:\n",
    "    for entry in train_sample:\n",
    "        f.write(json.dumps(simplify_jsonl(entry)) + \"\\n\")\n",
    "\n",
    "valid_sample = random.sample(x, 1000)\n",
    "with open(path_valid, 'w', encoding='utf-8') as f:\n",
    "    for entry in valid_sample:\n",
    "        f.write(json.dumps(simplify_jsonl(entry)) + \"\\n\")\n",
    "\n",
    "test_sample = random.sample(x, 1000)\n",
    "with open(path_test, 'w', encoding='utf-8') as f:\n",
    "    for entry in test_sample:\n",
    "        f.write(json.dumps(simplify_jsonl(entry)) + \"\\n\")"
   ],
   "id": "57e1e15d9a4326c5",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "a_chunk = random.sample(x, 1500)\n",
    "path_chunk = '/Users/artem.semidetnov/Documents/Predictor/train1/datachunk.jsonl'\n",
    "with open(path_chunk, 'w', encoding='utf-8') as f:\n",
    "    for entry in a_chunk:\n",
    "        f.write(json.dumps(simplify_jsonl(entry)) + \"\\n\")"
   ],
   "id": "b8888ec546c161c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip3 install mlx-lm",
   "id": "b739741ea1291aed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T12:29:53.476603Z",
     "start_time": "2025-11-26T12:29:52.341813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = 'mlx-community/codegemma-1.1-2b-4bit'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ],
   "id": "5014de51a886cf2f",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Embedding:\n\tsize mismatch for weight: copying a param with shape torch.Size([256000, 256]) from checkpoint, the shape in current model is torch.Size([256000, 2048]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmlx-community/codegemma-1.1-2b-4bit\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      4\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\n\u001B[0;32m----> 6\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/auto/auto_factory.py:604\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    602\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mconfig_class \u001B[38;5;241m==\u001B[39m config\u001B[38;5;241m.\u001B[39msub_configs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext_config\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    603\u001B[0m         config \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mget_text_config()\n\u001B[0;32m--> 604\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    605\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    608\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    609\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    610\u001B[0m )\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:277\u001B[0m, in \u001B[0;36mrestore_default_dtype.<locals>._wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m old_dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mget_default_dtype()\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    279\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_default_dtype(old_dtype)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:5048\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   5038\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   5039\u001B[0m         torch\u001B[38;5;241m.\u001B[39mset_default_dtype(dtype_orig)\n\u001B[1;32m   5041\u001B[0m     (\n\u001B[1;32m   5042\u001B[0m         model,\n\u001B[1;32m   5043\u001B[0m         missing_keys,\n\u001B[1;32m   5044\u001B[0m         unexpected_keys,\n\u001B[1;32m   5045\u001B[0m         mismatched_keys,\n\u001B[1;32m   5046\u001B[0m         offload_index,\n\u001B[1;32m   5047\u001B[0m         error_msgs,\n\u001B[0;32m-> 5048\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_pretrained_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5049\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5050\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5051\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheckpoint_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5052\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5053\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5054\u001B[0m \u001B[43m        \u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msharded_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5055\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5056\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5057\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5058\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5059\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5060\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5061\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5062\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5063\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5064\u001B[0m \u001B[38;5;66;03m# make sure token embedding weights are still tied if needed\u001B[39;00m\n\u001B[1;32m   5065\u001B[0m model\u001B[38;5;241m.\u001B[39mtie_weights()\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:5468\u001B[0m, in \u001B[0;36mPreTrainedModel._load_pretrained_model\u001B[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001B[0m\n\u001B[1;32m   5465\u001B[0m         args_list \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mtqdm(args_list, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading checkpoint shards\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   5467\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m args \u001B[38;5;129;01min\u001B[39;00m args_list:\n\u001B[0;32m-> 5468\u001B[0m         _error_msgs, disk_offload_index \u001B[38;5;241m=\u001B[39m \u001B[43mload_shard_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5469\u001B[0m         error_msgs \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m _error_msgs\n\u001B[1;32m   5471\u001B[0m \u001B[38;5;66;03m# Save offloaded index if needed\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:843\u001B[0m, in \u001B[0;36mload_shard_file\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m    841\u001B[0m \u001B[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001B[39;00m\n\u001B[1;32m    842\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_fsdp_enabled() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_local_dist_rank_0() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_quantized):\n\u001B[0;32m--> 843\u001B[0m     disk_offload_index \u001B[38;5;241m=\u001B[39m \u001B[43m_load_state_dict_into_meta_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    844\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    846\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshard_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreverse_key_renaming_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    850\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisk_offload_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    852\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    853\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    856\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m error_msgs, disk_offload_index\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py:120\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 120\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:770\u001B[0m, in \u001B[0;36m_load_state_dict_into_meta_model\u001B[0;34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001B[0m\n\u001B[1;32m    767\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_fsdp_enabled():\n\u001B[1;32m    768\u001B[0m         param_device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_local_dist_rank_0() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeta\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 770\u001B[0m     \u001B[43m_load_parameter_into_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam_device\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    773\u001B[0m     \u001B[38;5;66;03m# TODO naming is stupid it loads it as well\u001B[39;00m\n\u001B[1;32m    774\u001B[0m     hf_quantizer\u001B[38;5;241m.\u001B[39mcreate_quantized_param(model, param, param_name, param_device)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:667\u001B[0m, in \u001B[0;36m_load_parameter_into_model\u001B[0;34m(model, param_name, tensor)\u001B[0m\n\u001B[1;32m    665\u001B[0m module, param_type \u001B[38;5;241m=\u001B[39m get_module_from_name(model, param_name)\n\u001B[1;32m    666\u001B[0m \u001B[38;5;66;03m# This will check potential shape mismatch if skipped before\u001B[39;00m\n\u001B[0;32m--> 667\u001B[0m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[43mparam_type\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstrict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43massign\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:2624\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[1;32m   2616\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2617\u001B[0m             \u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m   2618\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2619\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)\n\u001B[1;32m   2620\u001B[0m             ),\n\u001B[1;32m   2621\u001B[0m         )\n\u001B[1;32m   2623\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2624\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   2625\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2626\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)\n\u001B[1;32m   2627\u001B[0m         )\n\u001B[1;32m   2628\u001B[0m     )\n\u001B[1;32m   2629\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for Embedding:\n\tsize mismatch for weight: copying a param with shape torch.Size([256000, 256]) from checkpoint, the shape in current model is torch.Size([256000, 2048])."
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                  # rank of LoRA matrices\n",
    "    lora_alpha=16,        # scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # layers to apply LoRA\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ],
   "id": "f9643642b20d5254"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"/Users/artem.semidetnov/Documents/Predictor/train1/train.jsonl\", \"validation\": \"/Users/artem.semidetnov/Documents/Predictor/train1/valid.jsonl\"})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mlx-community/codegemma-1.1-2b-4bit\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def preprocess(batch):\n",
    "    # Combine prompt and completion\n",
    "    combined_text = [p + tokenizer.eos_token + c for p, c in zip(batch[\"prompt\"], batch[\"completion\"])]\n",
    "    # Tokenize\n",
    "    return tokenizer(combined_text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)"
   ],
   "id": "de0c4ad16cf4ee02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(tokenized_dataset[\"train\"][0].keys())",
   "id": "7528201fd59729"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_mlxlm\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-4,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    bf16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "d134d668493d056c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# training?\n",
    "\n",
    "# model = 'mlx-community/codegemma-1.1-2b-4bit'\n",
    "model = 'mistralai/Mistral-7B-v0.1'\n",
    "dataset = '/Users/artem.semidetnov/Documents/Predictor/train1/'\n",
    "# output = './lora-output'\n",
    "\n",
    "!python3 -m mlx_lm lora --model {model} --data {dataset} --train --fine-tune-type lora --iters 1000 --steps-per-eval 100 --batch-size 4"
   ],
   "id": "d43312dc83356e95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train",
   "id": "33bfd49a82aee385"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T13:20:58.513074Z",
     "start_time": "2025-11-24T13:19:46.843388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = 'Qwen/Qwen2.5-Coder-0.5B-Instruct'\n",
    "dataset = '/Users/artem.semidetnov/Documents/Predictor/train1/'\n",
    "\n",
    "!python3 -m mlx_lm.lora --model {model} --data {dataset} --train --learning-rate 1e-5 --iters 100 --fine-tune-type lora --batch-size 1"
   ],
   "id": "f54ebd11aedda525",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/artem.semidetnov/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\r\n",
      "  warnings.warn(\r\n",
      "Calling `python -m mlx_lm.lora...` directly is deprecated. Use `mlx_lm.lora...` or `python -m mlx_lm lora ...` instead.\r\n",
      "Loading pretrained model\r\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 36291.88it/s]\r\n",
      "Loading datasets\r\n",
      "Training\r\n",
      "Trainable parameters: 0.594% (2.933M/494.033M)\r\n",
      "Starting training..., iters: 100\r\n",
      "Calculating loss...:   4%|▉                      | 1/25 [00:00<00:05,  4.69it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2232 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:  48%|██████████▌           | 12/25 [00:02<00:02,  5.75it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 3850 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:  80%|█████████████████▌    | 20/25 [00:04<00:01,  4.73it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 3308 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:  88%|███████████████████▎  | 22/25 [00:04<00:00,  3.93it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 4022 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...: 100%|██████████████████████| 25/25 [00:05<00:00,  4.47it/s]\r\n",
      "Iter 1: Val loss 2.403, Val took 5.599s\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2281 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 3969 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 8104 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2779 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Iter 10: Train loss 2.478, Learning Rate 1.000e-05, It/sec 1.069, Tokens/sec 1459.681, Trained Tokens 13658, Peak mem 8.447 GB\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2637 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 4867 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Iter 20: Train loss 2.334, Learning Rate 1.000e-05, It/sec 1.836, Tokens/sec 1798.928, Trained Tokens 23454, Peak mem 8.448 GB\r\n",
      "Iter 30: Train loss 1.887, Learning Rate 1.000e-05, It/sec 2.118, Tokens/sec 1809.140, Trained Tokens 31994, Peak mem 8.448 GB\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2652 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Iter 40: Train loss 1.723, Learning Rate 1.000e-05, It/sec 2.037, Tokens/sec 1798.688, Trained Tokens 40826, Peak mem 8.448 GB\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 3562 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 4298 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 5282 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Iter 50: Train loss 1.487, Learning Rate 1.000e-05, It/sec 1.574, Tokens/sec 1806.324, Trained Tokens 52301, Peak mem 8.448 GB\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 4788 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 4992 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 3906 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 8755 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2657 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Iter 60: Train loss 1.539, Learning Rate 1.000e-05, It/sec 1.388, Tokens/sec 1856.195, Trained Tokens 65670, Peak mem 8.448 GB\r\n",
      "Iter 70: Train loss 1.396, Learning Rate 1.000e-05, It/sec 4.815, Tokens/sec 1640.062, Trained Tokens 69076, Peak mem 8.448 GB\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 3686 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 3076 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Iter 80: Train loss 1.451, Learning Rate 1.000e-05, It/sec 2.107, Tokens/sec 1838.831, Trained Tokens 77803, Peak mem 8.448 GB\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2472 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Iter 90: Train loss 1.282, Learning Rate 1.000e-05, It/sec 1.909, Tokens/sec 1804.371, Trained Tokens 87254, Peak mem 8.448 GB\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 6640 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:   0%|                               | 0/25 [00:00<?, ?it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 5410 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:   4%|▉                      | 1/25 [00:00<00:11,  2.02it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2340 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:   8%|█▊                     | 2/25 [00:00<00:11,  2.08it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2200 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:  16%|███▋                   | 4/25 [00:01<00:07,  2.73it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2270 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:  44%|█████████▋            | 11/25 [00:03<00:03,  4.50it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 3119 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:  60%|█████████████▏        | 15/25 [00:04<00:02,  4.53it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2558 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:  88%|███████████████████▎  | 22/25 [00:06<00:00,  3.90it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 4022 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...: 100%|██████████████████████| 25/25 [00:07<00:00,  3.38it/s]\r\n",
      "Iter 100: Val loss 1.149, Val took 7.398s\r\n",
      "Iter 100: Train loss 1.292, Learning Rate 1.000e-05, It/sec 2.502, Tokens/sec 1827.569, Trained Tokens 94557, Peak mem 8.448 GB\r\n",
      "Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.\r\n",
      "Saved final weights to adapters/adapters.safetensors.\r\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T11:27:18.967165Z",
     "start_time": "2025-11-25T11:27:14.705524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = 'Qwen/Qwen2.5-Coder-0.5B-Instruct'\n",
    "adap_path = '/adapters_Qwen_small'\n",
    "test_prompt = r'func add(x y : Nat) : Nat'\n",
    "\n",
    "# !python3 -m mlx_lm.generate --model {model} --max-tokens 500 -adapters_Qwen_small-path {adap_path} --prompt {test_prompt}\n",
    "# prompt = '\\\\\\\\func add(x y : Nat) : Nat'\n",
    "# prompt= r'''['d : D.E', 'D : BottomJoinSemilattice', 'F : Functor D MonoidCat']<<<break>>>['| func-* {x y : E {Dom}} : func (x * y) = func x * func y', '| \\\\\\\\infixl 7 * E E : E', \\\"\\\\\\\\func \\\\\\\\infix 1 = {A : \\\\\\\\Type} (a a' : A) : \\\\\\\\Type => a = a'\\\", '| E : \\\\\\\\Set', '| Cod : BaseSet', '| func (E {Dom}) : E {Cod}', \\\"\\\\\\\\func inMap {D : BottomJoinSemilattice} {F1 : Functor D MonoidCat} (d : D.E) : MonoidHom (F.F d) (MonoidLatticeColimit {D} F1) {\\\\n  | func => \\\\\\\\lam (a : E {Dom {\\\\\\\\this}}) =>\\\\n  in~ {\\\\\\\\Sigma (j : D.Ob) (F j)} {\\\\\\\\lam (s : \\\\\\\\Sigma (j : D.Ob) (F j)) (s' : \\\\\\\\Sigma (j : D.Ob) (F j)) =>\\\\n    \\\\\\\\Sigma (p : D.Hom s.1 s'.1) (Func {s.1} {s'.1} p s.2 = s'.2)} (d, a)\\\\n} => \\\\\\\\new MonoidHom {\\\\n  | func-ide => {?hidden}\\\\n  | func-* => {?hidden}\\\\n}\\\", '| Dom : BaseSet']<<<break>>>\\\\Pi {x y : E {Dom {inMap {D} {F} d}}} ->\\n  func {inMap {D} {F} d} (x * y) = func {inMap {D} {F} d} x * func {inMap {D} {F} d} y'''\n",
    "\n",
    "prompt = r'''['d : D.E', 'D : BottomJoinSemilattice', 'F : Functor D MonoidCat']<<<break>>>['| func-* {x y : E {Dom}} : func (x * y) = func x * func y', '| \\\\\\\\infixl 7 * E E : E', \\\"\\\\\\\\func \\\\\\\\infix 1 = {A : \\\\\\\\Type} (a a' : A) : \\\\\\\\Type => a = a'\\\", '| E : \\\\\\\\Set', '| Cod : BaseSet', '| func (E {Dom}) : E {Cod}', \\\"\\\\\\\\func inMap {D : BottomJoinSemilattice} {F1 : Functor D MonoidCat} (d : D.E) : MonoidHom (F.F d) (MonoidLatticeColimit {D} F1) {\\\\n  | func => \\\\\\\\lam (a : E {Dom {\\\\\\\\this}}) =>\\\\n  in~ {\\\\\\\\Sigma (j : D.Ob) (F j)} {\\\\\\\\lam (s : \\\\\\\\Sigma (j : D.Ob) (F j)) (s' : \\\\\\\\Sigma (j : D.Ob) (F j)) =>\\\\n    \\\\\\\\Sigma (p : D.Hom s.1 s'.1) (Func {s.1} {s'.1} p s.2 = s'.2)} (d, a)\\\\n} => \\\\\\\\new MonoidHom {\\\\n  | func-ide => {?hidden}\\\\n  | func-* => {?hidden}\\\\n}\\\", '| Dom : BaseSet']<<<break>>>\\\\Pi {x y : E {Dom {inMap {D} {F} d}}} ->\\n  func {inMap {D} {F} d} (x * y) = func {inMap {D} {F} d} x * func {inMap {D} {F} d} y'''\n",
    "\n",
    "# prompt = prompt.replace('{', '(')\n",
    "# prompt = prompt.replace('}', ')')\n",
    "# prompt = prompt.replace('\\\\\\\\', '\\\\')\n",
    "\n",
    "\n",
    "!python3 -m mlx_lm.generate --model {model} --adapter-path {adap_path} --prompt \"{prompt}\""
   ],
   "id": "855da9bbe05dab9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/artem.semidetnov/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\r\n",
      "  warnings.warn(\r\n",
      "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py:127: RuntimeWarning: 'mlx_lm.generate' found in sys.modules after import of package 'mlx_lm', but prior to execution of 'mlx_lm.generate'; this may result in unpredictable behaviour\r\n",
      "  warn(RuntimeWarning(msg))\r\n",
      "Calling `python -m mlx_lm.generate...` directly is deprecated. Use `mlx_lm.generate...` or `python -m mlx_lm generate ...` instead.\r\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 19341.32it/s]\r\n",
      "==========\r\n",
      "\\lam (d : D.E) => func-* {inMap {D} {F} d}\r\n",
      "==========\r\n",
      "Prompt: 416 tokens, 2502.451 tokens-per-sec\r\n",
      "Generation: 23 tokens, 110.068 tokens-per-sec\r\n",
      "Peak memory: 1.805 GB\r\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:37:06.857352Z",
     "start_time": "2025-11-27T14:37:01.905225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# model_name = 'mlx-community/codegemma-1.1-2b-4bit'\n",
    "model_name = 'mlx-community/codegemma-1.1-2b-4bit'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "prompt_ = \"The tallest mountain in the world is\"\n",
    "inputs = tokenizer(prompt_, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# # Get logits\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(input_ids)\n",
    "#     logits = outputs.logits  # shape: [1, seq_len, vocab_size]\n",
    "#\n",
    "# # Get last token logits\n",
    "# last_token_logits = logits[0, -1]\n",
    "#\n",
    "# # Top 2 most probable tokens\n",
    "# topk = torch.topk(last_token_logits, k=5)\n",
    "# top_tokens = topk.indices\n",
    "# top_probs = torch.softmax(topk.values, dim=-1)\n",
    "#\n",
    "# # Decode\n",
    "# answers = [tokenizer.decode([token.item()]) for token in top_tokens]\n",
    "# print(\"Top answers:\", answers)\n",
    "# print(\"Probabilities:\", top_probs.tolist())\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=20,\n",
    "    num_beams=3,   # number of sequences to consider\n",
    "    num_return_sequences=2  # number of sequences to return\n",
    ")\n",
    "\n",
    "answers = [tokenizer.decode(seq, skip_special_tokens=True) for seq in outputs]\n",
    "for answer in answers:\n",
    "    print(answer)\n"
   ],
   "id": "bd56ae1c61476c4f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artem.semidetnov/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Embedding:\n\tsize mismatch for weight: copying a param with shape torch.Size([256000, 256]) from checkpoint, the shape in current model is torch.Size([256000, 2048]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmlx-community/codegemma-1.1-2b-4bit\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      6\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\n\u001B[0;32m----> 7\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m prompt_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe tallest mountain in the world is\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     10\u001B[0m inputs \u001B[38;5;241m=\u001B[39m tokenizer(prompt_, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/auto/auto_factory.py:604\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    602\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mconfig_class \u001B[38;5;241m==\u001B[39m config\u001B[38;5;241m.\u001B[39msub_configs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext_config\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    603\u001B[0m         config \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mget_text_config()\n\u001B[0;32m--> 604\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    605\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    608\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    609\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    610\u001B[0m )\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:277\u001B[0m, in \u001B[0;36mrestore_default_dtype.<locals>._wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m old_dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mget_default_dtype()\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    279\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_default_dtype(old_dtype)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:5048\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   5038\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   5039\u001B[0m         torch\u001B[38;5;241m.\u001B[39mset_default_dtype(dtype_orig)\n\u001B[1;32m   5041\u001B[0m     (\n\u001B[1;32m   5042\u001B[0m         model,\n\u001B[1;32m   5043\u001B[0m         missing_keys,\n\u001B[1;32m   5044\u001B[0m         unexpected_keys,\n\u001B[1;32m   5045\u001B[0m         mismatched_keys,\n\u001B[1;32m   5046\u001B[0m         offload_index,\n\u001B[1;32m   5047\u001B[0m         error_msgs,\n\u001B[0;32m-> 5048\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_pretrained_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5049\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5050\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5051\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheckpoint_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5052\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5053\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5054\u001B[0m \u001B[43m        \u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msharded_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5055\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5056\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5057\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5058\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5059\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5060\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5061\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5062\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5063\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5064\u001B[0m \u001B[38;5;66;03m# make sure token embedding weights are still tied if needed\u001B[39;00m\n\u001B[1;32m   5065\u001B[0m model\u001B[38;5;241m.\u001B[39mtie_weights()\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:5468\u001B[0m, in \u001B[0;36mPreTrainedModel._load_pretrained_model\u001B[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001B[0m\n\u001B[1;32m   5465\u001B[0m         args_list \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mtqdm(args_list, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading checkpoint shards\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   5467\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m args \u001B[38;5;129;01min\u001B[39;00m args_list:\n\u001B[0;32m-> 5468\u001B[0m         _error_msgs, disk_offload_index \u001B[38;5;241m=\u001B[39m \u001B[43mload_shard_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5469\u001B[0m         error_msgs \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m _error_msgs\n\u001B[1;32m   5471\u001B[0m \u001B[38;5;66;03m# Save offloaded index if needed\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:843\u001B[0m, in \u001B[0;36mload_shard_file\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m    841\u001B[0m \u001B[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001B[39;00m\n\u001B[1;32m    842\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_fsdp_enabled() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_local_dist_rank_0() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_quantized):\n\u001B[0;32m--> 843\u001B[0m     disk_offload_index \u001B[38;5;241m=\u001B[39m \u001B[43m_load_state_dict_into_meta_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    844\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    846\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshard_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreverse_key_renaming_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    850\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisk_offload_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    852\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    853\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    856\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m error_msgs, disk_offload_index\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py:120\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 120\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:770\u001B[0m, in \u001B[0;36m_load_state_dict_into_meta_model\u001B[0;34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001B[0m\n\u001B[1;32m    767\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_fsdp_enabled():\n\u001B[1;32m    768\u001B[0m         param_device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_local_dist_rank_0() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeta\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 770\u001B[0m     \u001B[43m_load_parameter_into_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam_device\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    773\u001B[0m     \u001B[38;5;66;03m# TODO naming is stupid it loads it as well\u001B[39;00m\n\u001B[1;32m    774\u001B[0m     hf_quantizer\u001B[38;5;241m.\u001B[39mcreate_quantized_param(model, param, param_name, param_device)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:667\u001B[0m, in \u001B[0;36m_load_parameter_into_model\u001B[0;34m(model, param_name, tensor)\u001B[0m\n\u001B[1;32m    665\u001B[0m module, param_type \u001B[38;5;241m=\u001B[39m get_module_from_name(model, param_name)\n\u001B[1;32m    666\u001B[0m \u001B[38;5;66;03m# This will check potential shape mismatch if skipped before\u001B[39;00m\n\u001B[0;32m--> 667\u001B[0m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[43mparam_type\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstrict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43massign\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:2624\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[1;32m   2616\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2617\u001B[0m             \u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m   2618\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2619\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)\n\u001B[1;32m   2620\u001B[0m             ),\n\u001B[1;32m   2621\u001B[0m         )\n\u001B[1;32m   2623\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2624\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   2625\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2626\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)\n\u001B[1;32m   2627\u001B[0m         )\n\u001B[1;32m   2628\u001B[0m     )\n\u001B[1;32m   2629\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for Embedding:\n\tsize mismatch for weight: copying a param with shape torch.Size([256000, 256]) from checkpoint, the shape in current model is torch.Size([256000, 2048])."
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ed0a7df0189d41ca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
