{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T13:04:15.345142Z",
     "start_time": "2025-11-24T13:04:15.339831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def parse_concatenated_json(path):\n",
    "    objects = []\n",
    "    buffer = []\n",
    "    brace_level = 0\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            # Count opening/closing braces not inside strings\n",
    "            brace_level += line.count(\"{\")\n",
    "            brace_level -= line.count(\"}\")\n",
    "\n",
    "            buffer.append(line)\n",
    "\n",
    "            # When brace level drops to zero we have a complete JSON object\n",
    "            if brace_level == 0 and buffer:\n",
    "                block = \"\".join(buffer).strip()\n",
    "                if block:\n",
    "                    objects.append(json.loads(block))\n",
    "                buffer = []\n",
    "\n",
    "    return objects\n"
   ],
   "id": "a94be2d13eaf37f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T13:04:22.871323Z",
     "start_time": "2025-11-24T13:04:21.579961Z"
    }
   },
   "cell_type": "code",
   "source": "x = parse_concatenated_json('/Users/artem.semidetnov/Documents/Predictor/data.json')",
   "id": "a2b70717e83b5f48",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T13:04:23.676343Z",
     "start_time": "2025-11-24T13:04:23.673171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def simplify_jsonl(entry : dict) -> dict:\n",
    "    two_field_dict = dict()\n",
    "    two_field_dict['prompt'] = str(entry['Context']) + \"<<<break>>>\" + str(entry['Premises']) + '<<<break>>>' + entry['Expected type']\n",
    "    two_field_dict['completion'] = entry['Expression']\n",
    "    return two_field_dict"
   ],
   "id": "f1bf19df79768904",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T13:06:46.026947Z",
     "start_time": "2025-11-24T13:06:45.974216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "path_train = '/Users/artem.semidetnov/Documents/Predictor/train1/train.jsonl'\n",
    "path_valid = '/Users/artem.semidetnov/Documents/Predictor/train1/valid.jsonl'\n",
    "path_test = '/Users/artem.semidetnov/Documents/Predictor/train1/test.jsonl'\n",
    "\n",
    "train_sample = random.sample(x, 1000)\n",
    "with open(path_train, 'w', encoding='utf-8') as f:\n",
    "    for entry in train_sample:\n",
    "        f.write(json.dumps(simplify_jsonl(entry)) + \"\\n\")\n",
    "\n",
    "valid_sample = random.sample(x, 100)\n",
    "with open(path_valid, 'w', encoding='utf-8') as f:\n",
    "    for entry in valid_sample:\n",
    "        f.write(json.dumps(simplify_jsonl(entry)) + \"\\n\")\n",
    "\n",
    "test_sample = random.sample(x, 100)\n",
    "with open(path_test, 'w', encoding='utf-8') as f:\n",
    "    for entry in test_sample:\n",
    "        f.write(json.dumps(simplify_jsonl(entry)) + \"\\n\")"
   ],
   "id": "57e1e15d9a4326c5",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "a_chunk = random.sample(x, 1500)\n",
    "path_chunk = '/Users/artem.semidetnov/Documents/Predictor/train1/datachunk.jsonl'\n",
    "with open(path_chunk, 'w', encoding='utf-8') as f:\n",
    "    for entry in a_chunk:\n",
    "        f.write(json.dumps(simplify_jsonl(entry)) + \"\\n\")"
   ],
   "id": "b8888ec546c161c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip3 install mlx-lm",
   "id": "b739741ea1291aed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = 'mlx-community/codegemma-1.1-2b-4bit'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ],
   "id": "5014de51a886cf2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                  # rank of LoRA matrices\n",
    "    lora_alpha=16,        # scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # layers to apply LoRA\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ],
   "id": "f9643642b20d5254"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"/Users/artem.semidetnov/Documents/Predictor/train1/train.jsonl\", \"validation\": \"/Users/artem.semidetnov/Documents/Predictor/train1/valid.jsonl\"})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mlx-community/codegemma-1.1-2b-4bit\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def preprocess(batch):\n",
    "    # Combine prompt and completion\n",
    "    combined_text = [p + tokenizer.eos_token + c for p, c in zip(batch[\"prompt\"], batch[\"completion\"])]\n",
    "    # Tokenize\n",
    "    return tokenizer(combined_text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)"
   ],
   "id": "de0c4ad16cf4ee02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(tokenized_dataset[\"train\"][0].keys())",
   "id": "7528201fd59729"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_mlxlm\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-4,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    bf16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "d134d668493d056c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# training?\n",
    "\n",
    "# model = 'mlx-community/codegemma-1.1-2b-4bit'\n",
    "model = 'mistralai/Mistral-7B-v0.1'\n",
    "dataset = '/Users/artem.semidetnov/Documents/Predictor/train1/'\n",
    "# output = './lora-output'\n",
    "\n",
    "!python3 -m mlx_lm lora --model {model} --data {dataset} --train --fine-tune-type lora --iters 1000 --steps-per-eval 100 --batch-size 4"
   ],
   "id": "d43312dc83356e95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train",
   "id": "33bfd49a82aee385"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T13:20:58.513074Z",
     "start_time": "2025-11-24T13:19:46.843388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = 'Qwen/Qwen2.5-Coder-0.5B-Instruct'\n",
    "dataset = '/Users/artem.semidetnov/Documents/Predictor/train1/'\n",
    "\n",
    "!python3 -m mlx_lm.lora --model {model} --data {dataset} --train --learning-rate 1e-5 --iters 100 --fine-tune-type lora --batch-size 1"
   ],
   "id": "f54ebd11aedda525",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/artem.semidetnov/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\r\n",
      "  warnings.warn(\r\n",
      "Calling `python -m mlx_lm.lora...` directly is deprecated. Use `mlx_lm.lora...` or `python -m mlx_lm lora ...` instead.\r\n",
      "Loading pretrained model\r\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 36291.88it/s]\r\n",
      "Loading datasets\r\n",
      "Training\r\n",
      "Trainable parameters: 0.594% (2.933M/494.033M)\r\n",
      "Starting training..., iters: 100\r\n",
      "Calculating loss...:   4%|▉                      | 1/25 [00:00<00:05,  4.69it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2232 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:  48%|██████████▌           | 12/25 [00:02<00:02,  5.75it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 3850 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:  80%|█████████████████▌    | 20/25 [00:04<00:01,  4.73it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 3308 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:  88%|███████████████████▎  | 22/25 [00:04<00:00,  3.93it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 4022 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...: 100%|██████████████████████| 25/25 [00:05<00:00,  4.47it/s]\r\n",
      "Iter 1: Val loss 2.403, Val took 5.599s\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2281 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 3969 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 8104 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2779 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Iter 10: Train loss 2.478, Learning Rate 1.000e-05, It/sec 1.069, Tokens/sec 1459.681, Trained Tokens 13658, Peak mem 8.447 GB\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2637 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 4867 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Iter 20: Train loss 2.334, Learning Rate 1.000e-05, It/sec 1.836, Tokens/sec 1798.928, Trained Tokens 23454, Peak mem 8.448 GB\r\n",
      "Iter 30: Train loss 1.887, Learning Rate 1.000e-05, It/sec 2.118, Tokens/sec 1809.140, Trained Tokens 31994, Peak mem 8.448 GB\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2652 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Iter 40: Train loss 1.723, Learning Rate 1.000e-05, It/sec 2.037, Tokens/sec 1798.688, Trained Tokens 40826, Peak mem 8.448 GB\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 3562 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 4298 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 5282 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Iter 50: Train loss 1.487, Learning Rate 1.000e-05, It/sec 1.574, Tokens/sec 1806.324, Trained Tokens 52301, Peak mem 8.448 GB\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 4788 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 4992 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 3906 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 8755 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2657 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Iter 60: Train loss 1.539, Learning Rate 1.000e-05, It/sec 1.388, Tokens/sec 1856.195, Trained Tokens 65670, Peak mem 8.448 GB\r\n",
      "Iter 70: Train loss 1.396, Learning Rate 1.000e-05, It/sec 4.815, Tokens/sec 1640.062, Trained Tokens 69076, Peak mem 8.448 GB\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 3686 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 3076 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Iter 80: Train loss 1.451, Learning Rate 1.000e-05, It/sec 2.107, Tokens/sec 1838.831, Trained Tokens 77803, Peak mem 8.448 GB\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2472 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Iter 90: Train loss 1.282, Learning Rate 1.000e-05, It/sec 1.909, Tokens/sec 1804.371, Trained Tokens 87254, Peak mem 8.448 GB\r\n",
      "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 6640 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:   0%|                               | 0/25 [00:00<?, ?it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 5410 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:   4%|▉                      | 1/25 [00:00<00:11,  2.02it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2340 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:   8%|█▊                     | 2/25 [00:00<00:11,  2.08it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2200 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:  16%|███▋                   | 4/25 [00:01<00:07,  2.73it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2270 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:  44%|█████████▋            | 11/25 [00:03<00:03,  4.50it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 3119 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:  60%|█████████████▏        | 15/25 [00:04<00:02,  4.53it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2558 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...:  88%|███████████████████▎  | 22/25 [00:06<00:00,  3.90it/s][WARNING] Some sequences are longer than 2048 tokens. The longest sentence 4022 will be truncated to 2048. Consider pre-splitting your data to save memory.\r\n",
      "Calculating loss...: 100%|██████████████████████| 25/25 [00:07<00:00,  3.38it/s]\r\n",
      "Iter 100: Val loss 1.149, Val took 7.398s\r\n",
      "Iter 100: Train loss 1.292, Learning Rate 1.000e-05, It/sec 2.502, Tokens/sec 1827.569, Trained Tokens 94557, Peak mem 8.448 GB\r\n",
      "Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.\r\n",
      "Saved final weights to adapters/adapters.safetensors.\r\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T11:27:18.967165Z",
     "start_time": "2025-11-25T11:27:14.705524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = 'Qwen/Qwen2.5-Coder-0.5B-Instruct'\n",
    "adap_path = '/Users/artem.semidetnov/Documents/Predictor/adapters'\n",
    "test_prompt = r'func add(x y : Nat) : Nat'\n",
    "\n",
    "# !python3 -m mlx_lm.generate --model {model} --max-tokens 500 -adapters-path {adap_path} --prompt {test_prompt}\n",
    "# prompt = '\\\\\\\\func add(x y : Nat) : Nat'\n",
    "# prompt= r'''['d : D.E', 'D : BottomJoinSemilattice', 'F : Functor D MonoidCat']<<<break>>>['| func-* {x y : E {Dom}} : func (x * y) = func x * func y', '| \\\\\\\\infixl 7 * E E : E', \\\"\\\\\\\\func \\\\\\\\infix 1 = {A : \\\\\\\\Type} (a a' : A) : \\\\\\\\Type => a = a'\\\", '| E : \\\\\\\\Set', '| Cod : BaseSet', '| func (E {Dom}) : E {Cod}', \\\"\\\\\\\\func inMap {D : BottomJoinSemilattice} {F1 : Functor D MonoidCat} (d : D.E) : MonoidHom (F.F d) (MonoidLatticeColimit {D} F1) {\\\\n  | func => \\\\\\\\lam (a : E {Dom {\\\\\\\\this}}) =>\\\\n  in~ {\\\\\\\\Sigma (j : D.Ob) (F j)} {\\\\\\\\lam (s : \\\\\\\\Sigma (j : D.Ob) (F j)) (s' : \\\\\\\\Sigma (j : D.Ob) (F j)) =>\\\\n    \\\\\\\\Sigma (p : D.Hom s.1 s'.1) (Func {s.1} {s'.1} p s.2 = s'.2)} (d, a)\\\\n} => \\\\\\\\new MonoidHom {\\\\n  | func-ide => {?hidden}\\\\n  | func-* => {?hidden}\\\\n}\\\", '| Dom : BaseSet']<<<break>>>\\\\Pi {x y : E {Dom {inMap {D} {F} d}}} ->\\n  func {inMap {D} {F} d} (x * y) = func {inMap {D} {F} d} x * func {inMap {D} {F} d} y'''\n",
    "\n",
    "prompt = r'''['d : D.E', 'D : BottomJoinSemilattice', 'F : Functor D MonoidCat']<<<break>>>['| func-* {x y : E {Dom}} : func (x * y) = func x * func y', '| \\\\\\\\infixl 7 * E E : E', \\\"\\\\\\\\func \\\\\\\\infix 1 = {A : \\\\\\\\Type} (a a' : A) : \\\\\\\\Type => a = a'\\\", '| E : \\\\\\\\Set', '| Cod : BaseSet', '| func (E {Dom}) : E {Cod}', \\\"\\\\\\\\func inMap {D : BottomJoinSemilattice} {F1 : Functor D MonoidCat} (d : D.E) : MonoidHom (F.F d) (MonoidLatticeColimit {D} F1) {\\\\n  | func => \\\\\\\\lam (a : E {Dom {\\\\\\\\this}}) =>\\\\n  in~ {\\\\\\\\Sigma (j : D.Ob) (F j)} {\\\\\\\\lam (s : \\\\\\\\Sigma (j : D.Ob) (F j)) (s' : \\\\\\\\Sigma (j : D.Ob) (F j)) =>\\\\n    \\\\\\\\Sigma (p : D.Hom s.1 s'.1) (Func {s.1} {s'.1} p s.2 = s'.2)} (d, a)\\\\n} => \\\\\\\\new MonoidHom {\\\\n  | func-ide => {?hidden}\\\\n  | func-* => {?hidden}\\\\n}\\\", '| Dom : BaseSet']<<<break>>>\\\\Pi {x y : E {Dom {inMap {D} {F} d}}} ->\\n  func {inMap {D} {F} d} (x * y) = func {inMap {D} {F} d} x * func {inMap {D} {F} d} y'''\n",
    "\n",
    "# prompt = prompt.replace('{', '(')\n",
    "# prompt = prompt.replace('}', ')')\n",
    "# prompt = prompt.replace('\\\\\\\\', '\\\\')\n",
    "\n",
    "\n",
    "!python3 -m mlx_lm.generate --model {model} --adapter-path {adap_path} --prompt \"{prompt}\""
   ],
   "id": "855da9bbe05dab9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/artem.semidetnov/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\r\n",
      "  warnings.warn(\r\n",
      "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py:127: RuntimeWarning: 'mlx_lm.generate' found in sys.modules after import of package 'mlx_lm', but prior to execution of 'mlx_lm.generate'; this may result in unpredictable behaviour\r\n",
      "  warn(RuntimeWarning(msg))\r\n",
      "Calling `python -m mlx_lm.generate...` directly is deprecated. Use `mlx_lm.generate...` or `python -m mlx_lm generate ...` instead.\r\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 19341.32it/s]\r\n",
      "==========\r\n",
      "\\lam (d : D.E) => func-* {inMap {D} {F} d}\r\n",
      "==========\r\n",
      "Prompt: 416 tokens, 2502.451 tokens-per-sec\r\n",
      "Generation: 23 tokens, 110.068 tokens-per-sec\r\n",
      "Peak memory: 1.805 GB\r\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bd56ae1c61476c4f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
