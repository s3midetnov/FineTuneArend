{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-27T15:57:26.837330Z",
     "start_time": "2025-11-27T15:57:26.835317Z"
    }
   },
   "source": "import ModelInference as mi",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T15:58:15.593976Z",
     "start_time": "2025-11-27T15:58:10.706911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out = mi.call_model(r'\\func f (x y : Bool)')\n",
    "print(out)"
   ],
   "id": "dc091b8576b07c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79fa12b763124e2abd184fdf1443e37c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\hlevels  {this} => x * x\n",
      "  | {f}, 0 => 0\n",
      "  | {f}, suc n => f n * f n + f n\n",
      "  | {f}, suc n => f n * f n + f n\n",
      "  | {f}, suc n => f n * f n + f n\n",
      "  | {f}, suc n => f n * f n + f n\n",
      "  | {f}, suc n => f n * f n + f n\n",
      "  | {f}, suc n => f n * f n + f n\n",
      "  | {f}, suc n => f n * f n + f n\n",
      "  | {f}, suc n => f n * f n + f n\n",
      "  | {f}, suc n => f n * f n + f n\n",
      "  | {f}, suc n => f n * f n + f n\n",
      "  | {f}, suc n => f n * f n + f n\n",
      "  | {f}, suc n => f n * f n + f n\n",
      "  | {f}, suc n => f n * f n + f n\n",
      "  | {f}, suc n => f n * f\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T11:35:58.619636Z",
     "start_time": "2025-12-01T11:35:58.401769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load base model\n",
    "base_model_name = \"mlx-community/Qwen2.5-Coder-1.5B-Instruct-Q6\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",  # Automatically handles device placement\n",
    ")\n",
    "print(\"here\")\n",
    "# Load LoRA adapter on top of base model\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"path/to/your/lora-adapter\"  # Directory with adapter_config.json and adapter_model.bin\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Your prompt here\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ],
   "id": "9d64a86be9035e81",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The model's quantization config from the arguments has no `quant_method` attribute. Make sure that the model has been correctly quantized",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Load base model\u001B[39;00m\n\u001B[1;32m      6\u001B[0m base_model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmlx-community/Qwen2.5-Coder-1.5B-Instruct-Q6\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 7\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbase_model_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat16\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Automatically handles device placement\u001B[39;49;00m\n\u001B[1;32m     11\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhere\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Load LoRA adapter on top of base model\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/auto/auto_factory.py:604\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    602\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mconfig_class \u001B[38;5;241m==\u001B[39m config\u001B[38;5;241m.\u001B[39msub_configs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext_config\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    603\u001B[0m         config \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mget_text_config()\n\u001B[0;32m--> 604\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    605\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    608\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    609\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    610\u001B[0m )\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:277\u001B[0m, in \u001B[0;36mrestore_default_dtype.<locals>._wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m old_dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mget_default_dtype()\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    279\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_default_dtype(old_dtype)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:4881\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   4872\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m transformers_explicit_filename\u001B[38;5;241m.\u001B[39mendswith(\n\u001B[1;32m   4873\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.safetensors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4874\u001B[0m     ) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m transformers_explicit_filename\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.safetensors.index.json\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   4875\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   4876\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe transformers file in the config seems to be incorrect: it is neither a safetensors file \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4877\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(*.safetensors) nor a safetensors index file (*.safetensors.index.json): \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4878\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtransformers_explicit_filename\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4879\u001B[0m         )\n\u001B[0;32m-> 4881\u001B[0m hf_quantizer, config, dtype, device_map \u001B[38;5;241m=\u001B[39m \u001B[43mget_hf_quantizer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muser_agent\u001B[49m\n\u001B[1;32m   4883\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4885\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gguf_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   4886\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   4887\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4888\u001B[0m     )\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/quantizers/auto.py:300\u001B[0m, in \u001B[0;36mget_hf_quantizer\u001B[0;34m(config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent)\u001B[0m\n\u001B[1;32m    298\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mget_hf_quantizer\u001B[39m(config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent):\n\u001B[1;32m    299\u001B[0m     pre_quantized \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mhasattr\u001B[39m(config, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquantization_config\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 300\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m pre_quantized \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mAutoHfQuantizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msupports_quant_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquantization_config\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    301\u001B[0m         pre_quantized \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    303\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m pre_quantized \u001B[38;5;129;01mor\u001B[39;00m quantization_config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/quantizers/auto.py:252\u001B[0m, in \u001B[0;36mAutoHfQuantizer.supports_quant_method\u001B[0;34m(quantization_config_dict)\u001B[0m\n\u001B[1;32m    250\u001B[0m     quant_method \u001B[38;5;241m=\u001B[39m QuantizationMethod\u001B[38;5;241m.\u001B[39mBITS_AND_BYTES \u001B[38;5;241m+\u001B[39m suffix\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m quant_method \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 252\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    253\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe model\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms quantization config from the arguments has no `quant_method` attribute. Make sure that the model has been correctly quantized\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    254\u001B[0m     )\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m quant_method \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m AUTO_QUANTIZATION_CONFIG_MAPPING:\n\u001B[1;32m    257\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[1;32m    258\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown quantization type, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquant_method\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - supported types are:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    259\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(AUTO_QUANTIZER_MAPPING\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Hence, we will skip the quantization. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo remove the warning, you can delete the quantization_config attribute in config.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    261\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: The model's quantization config from the arguments has no `quant_method` attribute. Make sure that the model has been correctly quantized"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a46efeac6d761038"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
