{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-27T14:49:54.843629Z",
     "start_time": "2025-11-27T14:49:54.840908Z"
    }
   },
   "source": [
    "from mlx_lm import load, generate\n",
    "from mlx_lm_lora.trainer.datasets import CacheDataset, ChatDataset\n",
    "from datasets import load_dataset\n",
    "from mlx_lm_lora.trainer.sft_trainer import SFTTrainingArgs, train_sft, evaluate_sft\n",
    "import mlx.optimizers as optim\n",
    "from mlx_lm.utils import save_config\n",
    "from mlx_lm.tuner.callbacks import TrainingCallback\n",
    "\n",
    "from pathlib import Path\n",
    "import path_file"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:08:42.148805Z",
     "start_time": "2025-11-27T14:08:40.680924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# At this point, the model has to be 'Instruct'\n",
    "model_name = \"mlx-community/Qwen2.5-Coder-1.5B-Instruct-Q6\"\n",
    "model, tokenizer = load(model_name)"
   ],
   "id": "c1c572056044cf8e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "010030c186cd4d43ad58d1d4c47a155d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:08:45.883727Z",
     "start_time": "2025-11-27T14:08:45.880972Z"
    }
   },
   "cell_type": "code",
   "source": "# test_out = generate(model, tokenizer, prompt=\"def f(x : int) -> str:\\n    \")",
   "id": "b272691a3e175769",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:08:48.267514Z",
     "start_time": "2025-11-27T14:08:48.265744Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_path = path_file.dataset_path",
   "id": "4503658e1c0b4ccd",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:10:20.828180Z",
     "start_time": "2025-11-27T14:10:20.826316Z"
    }
   },
   "cell_type": "code",
   "source": "# !python3 -m mlx_lm.lora --train --model mlx-community/Qwen2.5-Coder-1.5B-Instruct-Q6 --data train2 --batch-size 1\n",
   "id": "57c3bf898640c3be",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:48:23.714958Z",
     "start_time": "2025-11-27T14:48:23.707385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import mlx.core as mx\n",
    "import numpy as np\n",
    "\n",
    "def beam_search(model, tokenizer, prompt, beam_width=5, max_new_tokens=50):\n",
    "    # encode prompt\n",
    "    input_ids = mx.array([tokenizer.encode(prompt)])\n",
    "\n",
    "    # initialize beams\n",
    "    beams = [(input_ids, 0.0)]  # (tokens, logprob)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        new_beams = []\n",
    "\n",
    "        for tokens, logprob in beams:\n",
    "            logits = model(tokens).logits[:, -1, :]  # last token logits\n",
    "            probs = mx.softmax(logits, axis=-1)\n",
    "            probs = np.array(probs[0])  # to numpy for top-k\n",
    "            topk_ids = probs.argsort()[-beam_width:][::-1]  # best tokens\n",
    "\n",
    "            for tok in topk_ids:\n",
    "                tok_prob = np.log(probs[tok] + 1e-9)\n",
    "                new_tokens = mx.concatenate([tokens, mx.array([[tok]])], axis=1)\n",
    "                new_beams.append((new_tokens, logprob + tok_prob))\n",
    "\n",
    "        # keep only best beams\n",
    "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "        beams = new_beams[:beam_width]\n",
    "\n",
    "    # return final N sequences\n",
    "    results = [\n",
    "        tokenizer.decode(np.array(tokens[0].tolist()))\n",
    "        for (tokens, score) in beams\n",
    "    ]\n",
    "    return results\n"
   ],
   "id": "9b78a25fda90551d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:50:28.955995Z",
     "start_time": "2025-11-27T14:50:27.854569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model, tokenizer = load(\n",
    "    \"mlx-community/Qwen2.5-Coder-1.5B-Instruct-Q6\",\n",
    "    adapter_path='/Users/artem.semidetnov/Documents/Predictor/adapters'\n",
    ")"
   ],
   "id": "f58eab09d31caa26",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2d3da712c994bb0bc9fd36350afa178"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:50:30.792638Z",
     "start_time": "2025-11-27T14:50:30.757219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequences = beam_search(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=\"Explain quantum entanglement:\",\n",
    "    beam_width=3,\n",
    "    max_new_tokens=60\n",
    ")\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    print(f\"\\nSequence {i+1}:\")\n",
    "    print(seq)"
   ],
   "id": "36bab2c10f5708b8",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'array' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m sequences \u001B[38;5;241m=\u001B[39m \u001B[43mbeam_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mExplain quantum entanglement:\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeam_width\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m60\u001B[39;49m\n\u001B[1;32m      7\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, seq \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(sequences):\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mSequence \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[8], line 15\u001B[0m, in \u001B[0;36mbeam_search\u001B[0;34m(model, tokenizer, prompt, beam_width, max_new_tokens)\u001B[0m\n\u001B[1;32m     12\u001B[0m new_beams \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tokens, logprob \u001B[38;5;129;01min\u001B[39;00m beams:\n\u001B[0;32m---> 15\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogits\u001B[49m[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :]  \u001B[38;5;66;03m# last token logits\u001B[39;00m\n\u001B[1;32m     16\u001B[0m     probs \u001B[38;5;241m=\u001B[39m mx\u001B[38;5;241m.\u001B[39msoftmax(logits, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     17\u001B[0m     probs \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(probs[\u001B[38;5;241m0\u001B[39m])  \u001B[38;5;66;03m# to numpy for top-k\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'array' object has no attribute 'logits'"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6f380c6c4c6d8cf1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
